---
title: "Project 3 158: Multiple Linear Regression"
author: "Tesfa Asmara and Kevin Loun"
date: "4/09/2022"
output:
  bookdown::pdf_document2:
    extra_dependencies: ["float"]
bibliography: references.bib
---
```{r, include=FALSE}
library(broom)
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(tidyverse)
library(janitor)
library(ggridges)
library(ggplot2)
library(skimr)
knitr::opts_chunk$set(fig.pos="H", out.extra="")
library(patchwork)  # to get the plots next to one another
library(rsample)
library(GGally)
library(equatiomatic)
library(parsnip)
library(recipes)
library(workflows)
library(resample)
library(tidymodels)
library(gghighlight)
library(ggrepel)
library(car)
library(MASS)
```

# Introduction
The dataset for this project contains 10,000 League of Legends ranked matches from the North American region with 775 variables offered through the Riot Games API, provided on Kaggle [@riot][@james_2020]. Each match is pulled from players who rank Gold in the League system, a ranking system that matches players of a similar skill level to play with and against each other. Amongst North American players, the Gold skill level was the second most common tier, achieved by 27.7 percent of players, or approximately 49.86 million players when considered against Riot Games' player base of 180 million [@statista_2021][@riot_tweet]. This dataset will be referred to as $\texttt{lol10}$.

For this project, the following variables are of interest: time spent crowd controlling others, map side, longest time spent living, kills, gold earned, and total damage dealt. A figure including all the relevant variables and their description is attached at the end.

# Hypothesis
We consider the following research question: Are one or more of the independent variables, `b_gold_earned`, `b_kills`, `b_longest_time_spent_living`, or `b_time_c_cing_others` in the model useful in predicting the future values of `b_total_damage_dealt`?

# Feature Engineering
```{r echo = FALSE,fig.cap = "Correlation Matrix of Variables"}
# Read in the data
lol10 <- read_csv(show_col_types = FALSE, 'training_data.csv')
lol10 <- clean_names(lol10)

# Manipulate the data to extract the information we're interested in
lol10 <- lol10 %>%
  mutate(b_total_damage_dealt = b_summoner1_total_damage_dealt + b_summoner2_total_damage_dealt + b_summoner3_total_damage_dealt + b_summoner4_total_damage_dealt + b_summoner5_total_damage_dealt)
lol10 <- lol10 %>%
  mutate(b_gold_earned = b_summoner1_gold_earned + b_summoner2_gold_earned + b_summoner3_gold_earned + b_summoner4_gold_earned + b_summoner5_gold_earned)
lol10 <- lol10 %>%
  mutate(b_kills = b_summoner1_kills + b_summoner2_kills + b_summoner3_kills + b_summoner4_kills + b_summoner5_kills) 
lol10 <- lol10 %>%
  mutate(b_longest_time_spent_living = b_summoner1_longest_time_spent_living + b_summoner2_longest_time_spent_living + b_summoner3_longest_time_spent_living + b_summoner4_longest_time_spent_living + b_summoner5_longest_time_spent_living)
lol10 <- lol10 %>%
  mutate(b_time_c_cing_others = b_summoner1_time_c_cing_others + b_summoner2_time_c_cing_others + b_summoner3_time_c_cing_others + b_summoner4_time_c_cing_others + b_summoner5_time_c_cing_others)

lol10 <- lol10 %>% dplyr::select(b_total_damage_dealt, b_gold_earned, b_kills, b_longest_time_spent_living, b_time_c_cing_others)

# Separate the data for training and testing
set.seed(4747)
lol10_split <- initial_split(lol10) 
lol10_train <- training(lol10_split)
lol10_test  <- testing(lol10_split)

# pairs plot
ggpairs(lol10_train)
```
For $n$ = `r length(lol10_train)` observations, Table B.6 in ALSM is employed to assess whether or not the magnitude of the correlation coefficient supports the reasonableness of the normality assumption. The feature engineering we conducted was minimal.

# Interaction Variables
```{r echo = FALSE}
lol10_lm <- lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others, data=lol10_train)
lol10_i_lm <- lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others + b_gold_earned:b_kills + b_gold_earned:b_longest_time_spent_living + b_gold_earned:b_time_c_cing_others + b_kills:b_longest_time_spent_living + b_kills:b_time_c_cing_others + b_longest_time_spent_living:b_time_c_cing_others, data=lol10_train)
```
We wish to test formally in the `lol10` dataset whether interaction terms between the four explanatory variables should be included in the regression model. We therefore need to consider the following regression model: `r extract_eq(lol10_i_lm, wrap = TRUE, terms_per_line = 2, intercept = "beta")`.

We wish to test whether any interaction terms are needed. We do so by performing a partial F-test by fitting both the reduced and full models separately and thereafter comparing them using the `anova()` function. 
```{r echo = FALSE}
results <- anova(lol10_lm, lol10_i_lm)
```
Since $F \approx$ `r unlist(results["F"])[2]` (p-value $\approx$ `r unlist(results["Pr(>F)"])[2]`), we reject the null hypothesis $H_0: \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = \beta_{10} = 0$ at the $\alpha = 0.05$ level of significance. This means that the interaction terms do not contribute significant information to the `b_total_damage_dealt` once the explanatory variables `b_gold_earned, b_kills, b_longest_time_spent_living, b_time_c_cing_others` have been taken into consideration.

# Computational Model
From our domain experience, we consider the following parsiminous models: `r extract_eq(lm(b_total_damage_dealt ~ b_gold_earned + b_kills, data = lol10_train), wrap = TRUE, terms_per_line = 2, intercept = "beta")` and `r extract_eq(lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living, data = lol10_train), wrap = TRUE, terms_per_line = 2, intercept = "beta")`.

We compare the two models using cross-validation prediction error. 
```{r echo = FALSE}
lol10_spec <- linear_reg() %>%
  set_engine("lm")

lol10_rec1 <- recipe(b_total_damage_dealt ~ b_gold_earned + b_kills+b_time_c_cing_others, data = lol10_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

lol10_rec2 <- recipe(b_total_damage_dealt ~ b_gold_earned + b_kills +b_time_c_cing_others+ b_longest_time_spent_living, data = lol10_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

lol10_wflow_1 <- workflow() %>%
  add_model(lol10_spec) %>%
  add_recipe(lol10_rec1)
lol10_wflow_2 <- workflow() %>%
  add_model(lol10_spec) %>%
  add_recipe(lol10_rec2)

set.seed(47)
folds <- vfold_cv(lol10_train, v = 5)

lol10_comp_fit_rs_1 <- lol10_wflow_1 %>%
  fit_resamples(folds)

lol10_comp_fit_rs_2 <- lol10_wflow_2 %>%
  fit_resamples(folds)

cv_metrics1 <- collect_metrics(lol10_comp_fit_rs_1, summarize = FALSE)
cv_metrics2 <- collect_metrics(lol10_comp_fit_rs_2, summarize = FALSE)

cv_metrics1_summary <- cv_metrics1 %>%
  filter(.metric == "rmse") %>%
  summarise(
    min = min(.estimate),
    max = max(.estimate),
    mean = mean(.estimate),
    sd = sd(.estimate)
  )

cv_metrics2_summary <- cv_metrics2 %>%
  filter(.metric == "rmse") %>%
  summarise(
    min = min(.estimate),
    max = max(.estimate),
    mean = mean(.estimate),
    sd = sd(.estimate)
  )

lol10_train_summary <- lol10_train %>%
  summarise(
    min = min(b_total_damage_dealt),
    max = max(b_total_damage_dealt),
    mean = mean(b_total_damage_dealt),
    sd = sd(b_total_damage_dealt)
  )
```
In comparing which model is better, the CV RMSE provides information on how well the model did predicting each $1/v$, where $v = \text{the number of folds}$, hold out sample. We can compare the model RMSE to the original variability seen in the `b_total_damage_dealt` variable. The original variability (measured by standard deviation) of `b_total_damage_dealt` was `r lol10_train_summary$sd`. After running Model 1, the remaining variability (measured by RMSE averaged over the folds) is `r cv_metrics1_summary$sd`; after running Model 2, the remaining variability (measured by RMSE averaged over the folds) is `r cv_metrics2_summary$sd`.

Hence, the better computational model is `r extract_eq(lm(b_total_damage_dealt ~ b_gold_earned + b_kills, data = lol10_train), wrap = TRUE, terms_per_line = 2, intercept = "beta")`.

```{r echo = FALSE}
reg_summary <- summary(leaps::regsubsets(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others, data=lol10_train))
```

# Statistical Model
For the four predictors in the `lol10` data, we know there are $2^4 = 16$ possible models. The four-parameter model `r extract_eq(lol10_lm, wrap = TRUE, terms_per_line = 2, intercept = "beta")` is identified as best by the $R_{a, p}$ criterion; it has $\texttt{max}(R_{a, p}) =$ `r which.max(reg_summary$adjr2)` and will serve as the selected model.

```{r fig.cap = "R-squared and R-Squared Adjusted for Final Model", echo = FALSE}
selected_lm <- lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others, data = lol10_test)

test_results <- selected_lm %>%
  glance()
```
For the test data, the linear model we selected has $R^2$ = `r test_results$r.squared` and a $R^2_{adj}$ = `r test_results$adj.r.squared`. Therefore, `r (test_results$r.squared)/100`% of the variability in `b_total_damage_dealt` for players who rank Gold in the North American region is explained by the variables `b_gold_earned`, `b_kills`, `b_longest_time_spent_living`, `b_time_c_cing_others`. However, this $R^2$ is not a guarantee that our model will accurately describe the population. We have a relatively high $R^2$ value but this simply shows that the variables explain the variability but $R^2$ is easilly influenced by bias and can also be simply affected by the number of predictor variables in a model. So while $R^2$ is a good starting point it is not a guarantee that our model will be accurate to the population.

## Beta interpretation and Mean Value Confidence Interval
When looking closer at the $\beta{}$ coefficients of our model we can evaluate our coefficients based on their p-calue to determine if they are significant. Upon closer inspection it appears that `b_longest_time_spent_living` and `b_time_c_cing_others` are not significant in our final model. They have p-values of 0.208 and .608 respectively. We set our $\alpha{}$ to be 0.05 and as such these coefficients are not significant, however, `b_gold_earned` and `b_kills` are significant as they have p-values lower than our threshold.  
```{r echo=FALSE}
summary(selected_lm)%>%
  tidy()
```
```{r echo=FALSE, include=FALSE}
CI <-selected_lm%>%
  augment(interval="confidence")
CI
mean(CI$.lower)
mean(CI$.upper)
```
The 95% Confidence interval for total damage dealt for summoners on the blue side is (546714,555966) meaning that we are 95% confident that the true mean value of total damage dealt is between(546714,555966). A future predicted value for a combination of X's can be seen using the following combination of X's: 
```{r, echo=FALSE}
df <- data.frame("b_kills"=20, "b_gold_earned" = 55000, "b_longest_time_spent_living" = 60,"b_time_c_cing_others"=15, stringsAsFactors = FALSE)
df
```
And Once we apply the model we see that the new predicted value for future data containing similar values is:
```{r echo=FALSE}
predict(selected_lm,newdata=df)
```
## Model Interpretation 
The final model we selected indicates that `b_total_damage_dealt` can be predicted by `b_gold_earned`, `b_kills`, `b_longest_time_spent_living`, `b_time_c_cing_others`. Initially we began with a model that included interactions between these variables but they were insignificant and were dropped from the model. These were most likely non-significant because changing one variable did not have a strong affect on any other variable, that is an increase or decrease in one variable did not result in an increase or decrease in any other variabilty. There seems to be a high correlation between `b_gold_earned`and `b_kills` which can indicate multicollinearity or that one of these variables can take the place of each other. This issue of multicollinearity will be addressed through a Variance Inflation Factor test further in this report.

# Checking for Outliers
We first apply an outlier test on our data to see if there are any significant Bonferroni P values that could indicate potential outliers in our dataset. 
```{r, echo=FALSE}
# Assessing Outliers
outlierTest(selected_lm) # Bonferonni p-value for most extreme obs
#qqPlot(selected_lm, main="QQ Plot") #qq plot for studentized resid
```
The test output shows potential outliers at observations 1700,2495,1259,1972,413,335,1857,1063,1666,and 1665. While this test is important for identifying a potentially significant outlying observation, it is not the ultimate indicator of outliers and checking for patterns in our outliers so we can proceed to generate a plot of Dfbetas to see if these points reappear as outliers. 
```{r echo=FALSE,fig.cap = "Plots for DfBetas"}
plotdb<-dfbetaPlots(selected_lm, id.n=3) #Dfbetas
plotdb
```
We notice that the same outlying points appear on the dfbeta plots further indicating that these are outliers that could affect our model. In the leverage plot for each predictor we see a similar outcome; outliers at observation 1700 and 2495 for each predictor variable.

```{r, echo=FALSE, fig.cap = "Leverage Plots for Outliers"}
leveragePlots(selected_lm) # leverage plots
```
To ensure that the outliers aren't an underlying cause of high correlation between our variables, we ploted our data in an added variable plot to visualize our outliers in plots where opur predictors are independent of one another.
```{r echo=FALSE, fig.cap = "Added Variable Plot for each Predictor"}
# Influential Observations
# added variable plots
avPlots(selected_lm)
```
We then chose to utilzes Cook's Distance as a metric to evaluate our outlier points. We set our cutoff value as >$\frac{4}{(n-k-1)}$ , or 0.001601922. Any observation that yields a Cook's Distance value greater than our cutoff will be considered an outlier. 
```{r echo=FALSE, fig.cap = "Cook's Distance per Observation"}
# Cook's D plot
# identify D values > 4/(n-k-1)
cutoff <- 4/((nrow(lol10_test)-length(selected_lm$coefficients)-2))
plot(selected_lm, which=4, cook.levels=cutoff)
```

```{r,echo=FALSE, fig.cap = "Influence plot for Cook's Distance"}
# Influence Plot
influencePlot(selected_lm, main="Influence Plot", sub="Circle size is proportial to Cook's Distance")
# Checking for errors in data collection, etc
Checking<-lol10[c(496,1666,1700,2089,2495),c('b_gold_earned', 'b_kills', 'b_longest_time_spent_living','b_time_c_cing_others','b_total_damage_dealt')]
```
Based on the influence plot and our cutoff value, we can identify observations 496, 1666,1700,2089,and 2495 as outliers, and after checking to ensure that these observations being outliers were not a result of data errors or misformulated regression, we have decided to remove these outliers from our data. 

After removing the outliers from the data we can plot our studentized residuals in the form of a QQ plot and a histogram for datasets with and without the outliers. We noticed that without the outlier points our QQ plot appears more normal, though not perfect and that the shape of our histogram appears more normal than before droping the outliers.
```{r echo=FALSE,fig.cap ="QQ Plot and Distribution of Studentized Residuals"}
# Normality of Residuals
# qq plot for studentized resid
qqPlot(selected_lm, main="QQ Plot")
lol10_dropped<-lol10_test[-c(496,1666,1700,2089,2495,1259,1972,413,335,1857,1063,1659), ] #Drop outliers
edited_lm <-lm(formula = b_total_damage_dealt ~ b_gold_earned + b_kills + 
    b_longest_time_spent_living + b_time_c_cing_others, data = lol10_dropped)
qqPlot(edited_lm, main="QQ Plot")
#edited_lm drops outliers
# distribution of studentized residuals
sresid1 <- studres(selected_lm)
sresid2 <- studres(edited_lm)
hist(sresid1, freq=FALSE,
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid1),max(sresid1),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)

hist(sresid2, freq=FALSE,
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid2),max(sresid2),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
```

## Checking For Multicollinerity
In the correlation matrix generated earlier their appeared to be high correlation between some of our predictor variables; high correlation between predictor variables implies that there may be multicollinearity present in our model. To asses whether there really is multicollinearity we use the Variance Inflation factor for each variable. 
```{r, echo=FALSE}
# Evaluate Collinearity
vif(selected_lm) # variance inflation factors
```

```{r,echo=FALSE}
#create vector of VIF values
vif_values <- vif(selected_lm)

#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "steelblue", cex.names=.5,las=1)

```
We set our cutoff for VIF values at $VIF=>10$ and for our predictor variables the highest VIF was 5.308366 meaning that there is likely no multicollinearity occuring in our model.


# Summary 
We initially began by considering the following research question: Are one or more of the independent variables, `b_gold_earned`, `b_kills`, `b_longest_time_spent_living`, or `b_time_c_cing_others` in the model useful in predicting the future values of `b_total_damage_dealt`? We first began asnwering this question by assesing whether any feature engineering or interaction variables were neccessary in our model. We found that based on Table B.6 in ALSM that no feature engineering was needed. Through an F-test we found that non of the interaction variables were significant enough to add to our model. We then chose to approach the question through the use of a computational and statistical model. After comparing both models we concluded that the statistical model was the best choice for answering our question. We then proceeded to asses our data for any outliers that could affect our model; we accomplished this by using metrics such as: studentizeed residuals, cook's distance, dfbetas, and leverage. The outliers identified were not a result of data errors or misformulated regression so they were dropped from the data which in the end made the data more normal. Finally, a test for multicollinearity was applied due to a high correlation between variables. It was found that based on our cutoff value, no multicollinearity was present in our data. 

# Bibliography {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\noindent
<div id="refs"></div>


![Variables and their descriptions](./Variable2.pdf)
