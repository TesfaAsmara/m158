---
title: "Project 3 158: Multiple Linear Regression"
author: "Tesfa Asmara and Kevin Loun"
date: "4/09/2022"
output:
  bookdown::pdf_document2:
    extra_dependencies: ["float"]
bibliography: references.bib
---
```{r, include=FALSE}
library(broom)
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(tidyverse)
library(janitor)
library(ggridges)
library(ggplot2)
library(skimr)
knitr::opts_chunk$set(fig.pos="H", out.extra="")
library(patchwork)  # to get the plots next to one another
library(rsample)
library(GGally)
library(equatiomatic)
library(parsnip)
library(recipes)
library(workflows)
library(resample)
library(tidymodels)
library(gghighlight)
library(ggrepel)
library(car)
library(MASS)
```

# Introduction
The dataset for this project contains 10,000 League of Legends ranked matches from the North American region with 775 variables offered through the Riot Games API, provided on Kaggle [@riot][@james_2020]. Each match is pulled from players who rank Gold in the League system, a ranking system that matches players of a similar skill level to play with and against each other. Amongst North American players, the Gold skill level was the second most common tier, achieved by 27.7 percent of players, or approximately 49.86 million players when considered against Riot Games' player base of 180 million [@statista_2021][@riot_tweet]. This dataset will be referred to as $\texttt{lol10}$.

For this project, the following variables are of interest: time spent crowd controlling others, map side, longest time spent living, kills, gold earned, and total damage dealt. A figure including all the relevant variables and their description is attached at the end.

# Hypothesis
We consider the following research question: 

# Feature Engineering
```{r echo = FALSE}
# Read in the data
lol10 <- read_csv(show_col_types = FALSE, 'training_data.csv')
lol10 <- clean_names(lol10)

# Manipulate the data to extract the information we're interested in
lol10 <- lol10 %>%
  mutate(b_total_damage_dealt = b_summoner1_total_damage_dealt + b_summoner2_total_damage_dealt + b_summoner3_total_damage_dealt + b_summoner4_total_damage_dealt + b_summoner5_total_damage_dealt)
lol10 <- lol10 %>%
  mutate(b_gold_earned = b_summoner1_gold_earned + b_summoner2_gold_earned + b_summoner3_gold_earned + b_summoner4_gold_earned + b_summoner5_gold_earned)
lol10 <- lol10 %>%
  mutate(b_kills = b_summoner1_kills + b_summoner2_kills + b_summoner3_kills + b_summoner4_kills + b_summoner5_kills) 
lol10 <- lol10 %>%
  mutate(b_longest_time_spent_living = b_summoner1_longest_time_spent_living + b_summoner2_longest_time_spent_living + b_summoner3_longest_time_spent_living + b_summoner4_longest_time_spent_living + b_summoner5_longest_time_spent_living)
lol10 <- lol10 %>%
  mutate(b_time_c_cing_others = b_summoner1_time_c_cing_others + b_summoner2_time_c_cing_others + b_summoner3_time_c_cing_others + b_summoner4_time_c_cing_others + b_summoner5_time_c_cing_others)

lol10 <- lol10 %>% dplyr::select(b_total_damage_dealt, b_gold_earned, b_kills, b_longest_time_spent_living, b_time_c_cing_others)

# Separate the data for training and testing
set.seed(4747)
lol10_split <- initial_split(lol10) 
lol10_train <- training(lol10_split)
lol10_test  <- testing(lol10_split)

# pairs plot
ggpairs(lol10_train)
```
For $n$ = `r length(lol10_train)` observations, Table B.6 in ALSM is employed to assess whether or not the magnitude of the correlation coefficient supports the reasonableness of the normality assumption. The feature engineering we conducted was minimal.

# Interaction Variables
```{r echo = FALSE}
lol10_lm <- lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others, data=lol10_train)
lol10_i_lm <- lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others + b_gold_earned:b_kills + b_gold_earned:b_longest_time_spent_living + b_gold_earned:b_time_c_cing_others + b_kills:b_longest_time_spent_living + b_kills:b_time_c_cing_others + b_longest_time_spent_living:b_time_c_cing_others, data=lol10_train)
```
We wish to test formally in the `lol10` dataset whether interaction terms between the four explanatory variables should be included in the regression model. We therefore need to consider the following regression model: `r extract_eq(lol10_i_lm, wrap = TRUE, terms_per_line = 2, intercept = "beta")`.

We wish to test whether any interaction terms are needed. We do so by performing a partial F-test by fitting both the reduced and full models separately and thereafter comparing them using the `anova()` function. 
```{r echo = FALSE}
results <- anova(lol10_lm, lol10_i_lm)
```
Since $F \approx$ `r unlist(results["F"])[2]` (p-value $\approx$ `r unlist(results["Pr(>F)"])[2]`), we reject the null hypothesis $H_0: \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = \beta_{10} = 0$ at the $\alpha = 0.05$ level of significance. This means that the interaction terms do not contribute significant information to the `b_total_damage_dealt` once the explanatory variables `b_gold_earned, b_kills, b_longest_time_spent_living, b_time_c_cing_others` have been taken into consideration.

# Computational Model
From our domain experience, we consider the following parsiminous models: `r extract_eq(lm(b_total_damage_dealt ~ b_gold_earned + b_kills, data = lol10_train), wrap = TRUE, terms_per_line = 2, intercept = "beta")` and `r extract_eq(lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living, data = lol10_train), wrap = TRUE, terms_per_line = 2, intercept = "beta")`.

We compare the two models using cross-validation prediction error. 
```{r echo = FALSE}
lol10_spec <- linear_reg() %>%
  set_engine("lm")

lol10_rec1 <- recipe(b_total_damage_dealt ~ b_gold_earned + b_kills, data = lol10_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

lol10_rec2 <- recipe(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living, data = lol10_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

lol10_wflow_1 <- workflow() %>%
  add_model(lol10_spec) %>%
  add_recipe(lol10_rec1)
lol10_wflow_2 <- workflow() %>%
  add_model(lol10_spec) %>%
  add_recipe(lol10_rec2)

set.seed(47)
folds <- vfold_cv(lol10_train, v = 5)

lol10_comp_fit_rs_1 <- lol10_wflow_1 %>%
  fit_resamples(folds)

lol10_comp_fit_rs_2 <- lol10_wflow_2 %>%
  fit_resamples(folds)

cv_metrics1 <- collect_metrics(lol10_comp_fit_rs_1, summarize = FALSE)
cv_metrics2 <- collect_metrics(lol10_comp_fit_rs_2, summarize = FALSE)

cv_metrics1_summary <- cv_metrics1 %>%
  filter(.metric == "rmse") %>%
  summarise(
    min = min(.estimate),
    max = max(.estimate),
    mean = mean(.estimate),
    sd = sd(.estimate)
  )

cv_metrics2_summary <- cv_metrics2 %>%
  filter(.metric == "rmse") %>%
  summarise(
    min = min(.estimate),
    max = max(.estimate),
    mean = mean(.estimate),
    sd = sd(.estimate)
  )

lol10_train_summary <- lol10_train %>%
  summarise(
    min = min(b_total_damage_dealt),
    max = max(b_total_damage_dealt),
    mean = mean(b_total_damage_dealt),
    sd = sd(b_total_damage_dealt)
  )
```
In comparing which model is better, the CV RMSE provides information on how well the model did predicting each $1/v$, where $v = \text{the number of folds}$, hold out sample. We can compare the model RMSE to the original variability seen in the `b_total_damage_dealt` variable. The original variability (measured by standard deviation) of `b_total_damage_dealt` was `r lol10_train_summary$sd`. After running Model 1, the remaining variability (measured by RMSE averaged over the folds) is `r cv_metrics1_summary$sd`; after running Model 2, the remaining variability (measured by RMSE averaged over the folds) is `r cv_metrics2_summary$sd`.

Hence, the better computational model is `r extract_eq(lm(b_total_damage_dealt ~ b_gold_earned + b_kills, data = lol10_train), wrap = TRUE, terms_per_line = 2, intercept = "beta")`.

```{r echo = FALSE}
reg_summary <- summary(leaps::regsubsets(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others, data=lol10_train))
```

# Statistical Model
For the four predictors in the `lol10` data, we know there are $2^4 = 16$ possible models. The four-parameter model `r extract_eq(lol10_lm, wrap = TRUE, terms_per_line = 2, intercept = "beta")` is identified as best by the $R_{a, p}$ criterion; it has $\texttt{max}(R_{a, p}) =$ `r which.max(reg_summary$adjr2)` and will serve as the selected model.

```{r fig.cap = "R-squared and R-Squared Adjusted for Final Model", echo = FALSE}
selected_lm <- lm(b_total_damage_dealt ~ b_gold_earned + b_kills + b_longest_time_spent_living + b_time_c_cing_others, data = lol10_test)

test_results <- selected_lm %>%
  glance()
```
For the test data, the linear model we selected has $R^2$ = `r test_results$r.squared` and a $R^2_{adj}$ = `r test_results$adj.r.squared`. Therefore, `r (test_results$r.squared)/100`% of the variability in `b_total_damage_dealt` for players who rank Gold in the North American region is explained by the variables `b_gold_earned, b_kills, b_longest_time_spent_living, b_time_c_cing_others`.

```{r}
# Assessing Outliers
outlierTest(selected_lm) # Bonferonni p-value for most extreme obs
qqPlot(selected_lm, main="QQ Plot") #qq plot for studentized resid
leveragePlots(selected_lm) # leverage plots
```

```{r}
# Influential Observations
# added variable plots
avPlots(selected_lm)
# Cook's D plot
# identify D values > 4/(n-k-1)
cutoff <- 4/((nrow(lol10_test)-length(selected_lm$coefficients)-2))
plot(selected_lm, which=4, cook.levels=cutoff)
# Influence Plot
influencePlot(selected_lm, main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```
```{r}
# Normality of Residuals
# qq plot for studentized resid
qqPlot(selected_lm, main="QQ Plot")
# distribution of studentized residuals
sresid <- studres(selected_lm)
hist(sresid, freq=FALSE,
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
```
```{r}
# Evaluate Collinearity
vif(selected_lm) # variance inflation factors
```


Interpret your $\beta$ coefficients to the best of your ability.
Report the $R^2$ and Adjusted-$R^2$ values on the test data.
A complete analysis of the residuals and influence points.
Try to give an interpretation of the model that makes sense.
Give CIs for a mean predicted value and a future predicted value for at least one combination of X’s (from your final linear model).
Summarize your report.
Add pretty language.
Fix Variables and their descriptions table for MLR.
Report on any relationships between the explanatory variables.

![Variables and their descriptions](./Variables_and_their_descriptions.pdf)

# Bibliography {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\noindent
<div id="refs"></div>
